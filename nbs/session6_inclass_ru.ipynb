{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6 - Benchmark classification on ```cifar-10```\n",
    "\n",
    "This notebook builds on what we were doing last week with the handwritten digits from the MNIST dataset.\n",
    "\n",
    "This week, we're working with another famous dataset in computer vision and image processing research - [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path tools\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# data loader\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# machine learning tools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# classificatio models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7343d4b5",
   "metadata": {},
   "source": [
    "We're going to load the data using a function from the library ```TensorFlow```, which we'll be looking at in more detail next week. \n",
    "\n",
    "For now, we're just using it to fetch the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() #making it a tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b588be73",
   "metadata": {},
   "source": [
    "**Question:** What is the shape of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # 4 dimensional numpy array  (n objects, 32*32 size, 3 colour channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a08115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [9],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train # these are the current labels, they suck, they are index (so 9= truck, 6 =frog, a lil dumb, lets remake them)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd02fcbe",
   "metadata": {},
   "source": [
    "Unfortunately, this version of the data set doesn't have explict labels, so we need to create our own.\n",
    "Labels can be found here in documentation of data: https://www.cs.toronto.edu/~kriz/cifar.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"airplane\", \n",
    "          \"automobile\", \n",
    "          \"bird\", \n",
    "          \"cat\", \n",
    "          \"deer\", \n",
    "          \"dog\", \n",
    "          \"frog\", \n",
    "          \"horse\", \n",
    "          \"ship\", \n",
    "          \"truck\"] #note this is alfabetically "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all the data to greyscale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f5391f3",
   "metadata": {},
   "source": [
    "In the following cell, I'm converting all of my images to greyscale and then making a ```numpy``` array at the end.\n",
    "\n",
    "Notice that I'm using something funky here called *[list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#list comprehensions (a loop in a single line)\n",
    "X_train_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_train]) #list \n",
    "X_test_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab8a78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_grey.shape # now its a 3 dimensionel object (50000 images, 32 width, 32 height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd40289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list comprehension (translation)\n",
    "#for x in y:\n",
    "#    do_this(x)\n",
    "\n",
    "#[do_this(x) for x in y]\n",
    "#OR\n",
    "#[x.upper() for x in y]\n",
    "#EG, having the list colours = [\"red\", \"green\", \"blue\"]\n",
    "# upper = [colour.upper() for colour in colours] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9703dbdc",
   "metadata": {},
   "source": [
    "Then, we're going to do some simple scaling by dividing by 255.\n",
    "- We're essentially doing this to make smaller numbers and fit the data better.\n",
    "- Every pixel value in each 50000 32x32 image is now compressed! (dimensions are the same, we still have 50000 32x32 images, but now with LOWER pixel values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_scaled = (X_train_grey)/255.0 \n",
    "X_test_scaled = (X_test_grey)/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c141a5e2",
   "metadata": {},
   "source": [
    "Next, we're going to reshape this data. \n",
    "- Both of the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1024)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsamples, nx, ny = X_train_scaled.shape #extracting each dimension (samples, 32, 32)\n",
    "X_train_dataset = X_train_scaled.reshape((nsamples,nx*ny)) #we are shaping our data from 3D (50000, 32,32) to 2D (50000, 1024)\n",
    "\n",
    "#this is litteraly FLATTENING IT (as we have done before apparently)\n",
    "#imagine having a pic of a car, that we want to flatten to one hidden layer where each node is a pixel in the 32*32 carpicture. \n",
    "# and now the data went from 32*32 pic to one hidden layer of 1024 nodes for each 50000 image\n",
    "\n",
    "X_train_dataset.shape #see now we have 2D \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_test_scaled.shape\n",
    "X_test_dataset = X_test_scaled.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple logistic regression classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15bdea84",
   "metadata": {},
   "source": [
    "We define our Logistic Regression classifier as we have done previously. You'll notice that I've set a lot of different parameters here - you can learn more in the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "- PENALTY:\n",
    "    - L1 = setting very small (close to 0) weights to 0, and only keeping only meaingful weights\n",
    "    - none = no penalty\n",
    "- TOL:\n",
    "    - toleance for stopping criteria. When the model stop learning (improving the weight values), then stop training. \n",
    "    - Default is usually pretty small, so 0.1 is a little high actually \n",
    "- VERBOSE: \n",
    "    - Boolean value. \n",
    "    - Default is FALSE: meaing no ouput is printet\n",
    "    - TRUE: output is printed as it runs\n",
    " \n",
    "- SOLVER: \n",
    "    - algorithm for optimization problem.\n",
    "    - See documentation when choosing (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "    - Depends on dataset size and multiclass problems. \n",
    "\n",
    "- MULTI_CLASS: \n",
    "    - either multinomial and binary. \n",
    "    - In this case we have multinomial data, and not (FAKE vs REAL data articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.31719215\n",
      "Epoch 3, change: 0.13990186\n",
      "convergence after 4 epochs took 18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   17.6s finished\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty=\"none\", \n",
    "                        tol=0.1,\n",
    "                        verbose=True,\n",
    "                        solver=\"saga\",\n",
    "                        multi_class=\"multinomial\").fit(X_train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc10cdb4",
   "metadata": {},
   "source": [
    "We can then print our classification report, using the label names that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.34      0.39      0.36      1000\n",
      "  automobile       0.38      0.37      0.37      1000\n",
      "        bird       0.28      0.20      0.23      1000\n",
      "         cat       0.24      0.16      0.19      1000\n",
      "        deer       0.25      0.25      0.25      1000\n",
      "         dog       0.30      0.29      0.30      1000\n",
      "        frog       0.28      0.30      0.29      1000\n",
      "       horse       0.33      0.31      0.32      1000\n",
      "        ship       0.33      0.40      0.36      1000\n",
      "       truck       0.37      0.48      0.42      1000\n",
      "\n",
      "    accuracy                           0.32     10000\n",
      "   macro avg       0.31      0.32      0.31     10000\n",
      "weighted avg       0.31      0.32      0.31     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels) #cool classification report function\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79f6d9b4",
   "metadata": {},
   "source": [
    "I've set a couple of different parameters here - you can see more in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "**NB!** This will take a long time to run! On the 32 CPU machine on UCloud, this takes around 30 seconds per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=42,\n",
    "                    hidden_layer_sizes=(64, 10),\n",
    "                    learning_rate=\"adaptive\",\n",
    "                    early_stopping=True,\n",
    "                    verbose=True,\n",
    "                    max_iter=20).fit(X_train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e489977e",
   "metadata": {},
   "source": [
    "Lastly, we can get our classification report as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a5067ab",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Take the code outlined in this notebook and turn it into two separate Python scripts, one which performs Logistic Regression classification and one which uses the MLPClassifier on the ```Cifar10``` dataset.\n",
    "\n",
    "Try to use the things we've spoken about in clas\n",
    "- Requirements.txt\n",
    "- Virtual environment\n",
    "- Setup scripts\n",
    "- Argparse\n",
    "\n",
    "This task is [Assignment 2 for Visual Analytics](https://classroom.github.com/a/KLVvny7d)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
